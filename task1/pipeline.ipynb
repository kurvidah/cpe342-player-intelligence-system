{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Task 1 Pipeline\n",
    "\n",
    " This modified notebook integrates **Optuna** for automated hyperparameter tuning and **correlation-based feature selection** into the `scikit-learn` pipeline structure.\n",
    "\n",
    " **Key Additions:**\n",
    " 1.  **Optuna:** Used to efficiently search for the best `XGBClassifier` hyperparameters.\n",
    " 2.  **Feature Selection:** Implemented inside the Optuna objective function to allow the model to try different feature subsets based on their correlation with the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import roc_auc_score, make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Load Data and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "def feature_engineer(data):\n",
    "    epsilon = 1e-6 # Small constant to prevent division by zero\n",
    "    data['reports_per_day'] = data['reports_received'] / (data['account_age_days'] + epsilon)\n",
    "    data['kdr_x_hs'] = data['kill_death_ratio'] * data['headshot_percentage']\n",
    "    data['cheating_skill_metric'] = data['accuracy_score'] * data['headshot_percentage'] * data['spray_control_score']\n",
    "    \n",
    "    soft_skill_sum = data['game_sense_score'] + data['team_play_score']\n",
    "    data['skill_discrepancy'] = (data['headshot_percentage'] + data['accuracy_score']) / (soft_skill_sum + epsilon)\n",
    "    data['normalized_reaction_time'] = data['reaction_time_ms'] * data['first_blood_rate']\n",
    "    data['stealth_score'] = data['utility_usage_rate'] / (data['movement_pattern_score'] + epsilon)\n",
    "    data['technical_red_flags'] = data['device_changes_count'] / (data['input_consistency_score'] + epsilon)\n",
    "    \n",
    "    perf_growth_product = data['level_progression_speed'] * data['kill_consistency']\n",
    "    data['performance_vs_longevity'] = perf_growth_product / (data['account_age_days'] + epsilon)\n",
    "    data['social_isolation_index'] = data['reports_received'] / (data['friend_network_size'] + epsilon)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Apply feature engineering to both training and test data\n",
    "df = feature_engineer(df)\n",
    "test_df = feature_engineer(test_df)\n",
    "\n",
    "# Drop rows where the target is missing (if any)\n",
    "df = df.dropna(subset=['is_cheater'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Define Features and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (78198, 40)\n",
      "Validation set shape: (19550, 40)\n"
     ]
    }
   ],
   "source": [
    "# Assuming all columns except 'is_cheater', 'player_id', and 'id' are potential features.\n",
    "initial_features = [col for col in df.columns if col not in ['is_cheater', 'player_id', 'id']]\n",
    "X = df[initial_features]\n",
    "y = df['is_cheater']\n",
    "\n",
    "# features = ['reports_received', 'crosshair_placement', 'kdr_x_hs', 'cheating_skill_metric', 'headshot_percentage', 'kill_death_ratio', 'game_sense_score', 'account_age_days', 'accuracy_score', 'reports_per_day', 'level', 'spray_control_score', 'friend_network_size', 'win_rate', 'aiming_smoothness', 'level_progression_speed', 'kill_consistency', 'reaction_time_ms']\n",
    "features = [\n",
    "    'crosshair_placement',\n",
    "    'reports_received',\n",
    "    'account_age_days',\n",
    "    'friend_network_size',\n",
    "    'level',\n",
    "    'kdr_x_hs',\n",
    "    'cheating_skill_metric',\n",
    "    'reports_per_day'\n",
    "]\n",
    "\n",
    "# Split Data (critical for preventing leakage)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set shape: {X_train.shape}')\n",
    "print(f'Validation set shape: {X_val.shape}')\n",
    "\n",
    "# Calculate the scale_pos_weight for XGBoost to handle class imbalance\n",
    "scale_pos_weight = y_train.value_counts()[0] / y_train.value_counts()[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## 4. Optuna Objective Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # --- 4.1 Feature Selection (Correlation-based) ---\n",
    "    # Combine X_train and y_train to compute correlations\n",
    "    train_data = X_train.copy()\n",
    "    train_data['is_cheater'] = y_train\n",
    "    \n",
    "    # Prepare data subset for the pipeline\n",
    "    X_train_sub = X_train[features]\n",
    "    X_val_sub = X_val[features]\n",
    "\n",
    "    # --- 4.2 Hyperparameter Tuning for XGBoost ---\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000, step=50),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-4, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n",
    "    }\n",
    "\n",
    "    # --- 4.3 Define Preprocessing Pipeline for Selected Features ---\n",
    "    # The preprocessor is only applied to the selected features\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', IterativeImputer(random_state=42)),\n",
    "        ('scaler', RobustScaler())\n",
    "    ])\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "    \n",
    "    # --- 4.4 Create and Train the Full Pipeline ---\n",
    "    model_pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', XGBClassifier(\n",
    "            random_state=42, \n",
    "            use_label_encoder=False, \n",
    "            eval_metric='logloss',\n",
    "            scale_pos_weight=scale_pos_weight,\n",
    "            **param\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    # Train the pipeline\n",
    "    model_pipeline.fit(X_train_sub, y_train)\n",
    "\n",
    "    # Evaluate on the validation set. ROC AUC is a robust metric for imbalanced classification.\n",
    "    y_pred_proba = model_pipeline.predict_proba(X_val_sub)[:, 1]\n",
    "    roc_auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    return roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## 5. Run Optuna Study\n",
    "\n",
    "\n",
    "\n",
    " We run the Optuna study to find the best combination of feature selection and XGBoost hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\", study_name=\"xgb_pipeline_tuning\", load_if_exists=True)\n",
    "study.optimize(objective, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"\\n--- Optuna Study Results ---\")\n",
    "print(f\"Number of finished trials: {len(study.trials)}\")\n",
    "print(f\"Best ROC AUC Score: {best_trial.value:.4f}\")\n",
    "print(\"Best Hyperparameters:\")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## 6. Build and Train the Final Best Model\n",
    "\n",
    "\n",
    "\n",
    " We now use the best settings found by Optuna to train the final model on the full training set (X_train, y_train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bypass Tuning\n",
    "best_params = {\n",
    "    'n_estimators': 1000,\n",
    "    'max_depth': 8,\n",
    "    'learning_rate': 0.007816711717369125,\n",
    "    'subsample': 0.7415444084261885,\n",
    "    'colsample_bytree': 0.6755835023784938,\n",
    "    'gamma': 0.00013918059131830187,\n",
    "    'min_child_weight': 9,\n",
    "    'reg_lambda': 4.893454406264408e-06\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training the final best model pipeline...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schpudt/Workspace/cpe342-player-intelligence-system/.venv/lib/python3.13/site-packages/xgboost/training.py:199: UserWarning: [18:51:41] WARNING: /workspace/src/learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Separate best hyperparameters and the correlation threshold\n",
    "best_params = {k: v for k, v in best_trial.params.items() if k != 'corr_threshold'}\n",
    "\n",
    "# --- Final Preprocessing Pipeline for the Best Feature Set ---\n",
    "# The preprocessor only needs to know about the selected features\n",
    "numeric_transformer_final = Pipeline(steps=[\n",
    "    ('imputer', IterativeImputer(random_state=42)),\n",
    "    ('scaler', RobustScaler())\n",
    "])\n",
    "\n",
    "preprocessor_final = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer_final, features)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# --- Final Model Pipeline ---\n",
    "final_model_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_final),\n",
    "    ('classifier', XGBClassifier(\n",
    "        random_state=42, \n",
    "        use_label_encoder=False, \n",
    "        eval_metric='logloss', \n",
    "        scale_pos_weight=scale_pos_weight, \n",
    "        **best_params\n",
    "    ))\n",
    "])\n",
    "\n",
    "# Train the final pipeline on the selected features\n",
    "print(\"\\nTraining the final best model pipeline...\")\n",
    "final_model_pipeline.fit(X_train[features], y_train)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Evaluate the Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating the final model on the validation set...\n",
      "ROC AUC Score: 0.883809966018385\n",
      "F1 Score: 0.7095189600431319\n",
      "Accuracy: 0.7519693094629156\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.69      0.78     12724\n",
      "         1.0       0.60      0.87      0.71      6826\n",
      "\n",
      "    accuracy                           0.75     19550\n",
      "   macro avg       0.75      0.78      0.75     19550\n",
      "weighted avg       0.80      0.75      0.76     19550\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEvaluating the final model on the validation set...\")\n",
    "# Ensure validation data only contains the selected features\n",
    "X_val_sub = X_val[features]\n",
    "\n",
    "y_pred = final_model_pipeline.predict(X_val_sub)\n",
    "y_pred_proba = final_model_pipeline.predict_proba(X_val_sub)[:, 1]\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_val, y_pred_proba))\n",
    "print(\"F1 Score:\", f1_score(y_val, y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Make Predictions on the Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_sub = test_df[features] \n",
    "\n",
    "# The pipeline automatically applies all the same preprocessing steps\n",
    "test_predictions = final_model_pipeline.predict(test_features_sub)\n",
    "\n",
    "# Create submission file\n",
    "submission_df = pd.DataFrame({'id': test_df['id'], 'is_cheater': test_predictions})\n",
    "submission_df.to_csv('submission_optuna.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
